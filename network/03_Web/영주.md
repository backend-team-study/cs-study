# 03. Web

> 김영주

<br>

## 3.1. Web Server VS WAS

### Web Server

- HTTP 프로토콜을 기반으로 클라이언트의 요청을 서비스하는 기능을 담당한다.
  - 정적인 컨텐츠를 제공한다. (ex. html, css 등) -> 이때는 WAS를 거치지 않고 바로 컨텐츠를 제공한다.
  - 동적인 컨텐츠 제공을 위한 요청을 전달한다. -> 이때는 클라이언트의 요청을 WAS에게 보내고, WAS의 처리 결과를 클라이언트에게 응답한다.
- 정적 파일에 대해서는 WAS까지 갈 필요 없이 웹 서버 단에서 응답하여 애플리케이션 서버의 부담을 줄여줄 수 있다.
- Apache, NginX, IIS 등이 있다.

<br>

### WAS (Web Application Server)

- 다양한 로직 처리를 요구하는 동적 컨텐츠를 제공하는 기능을 담당한다.
- 따라서 WAS를 통해 요청에 맞는 데이터를 DB에서 가져와서 비즈니스 로직에 맞게 결과를 만들어 제공하여 자원을 효율적으로 사용할 수 있게 된다.
- HTTP 프로토콜을 통해 컴퓨터나 장치의 애플리케이션을 수행해주는 미들웨어이다.
- 웹 컨테이너 혹은 서블릿 컨테이너라고도 불린다. (WAS = Web Server + Web Container)
- Tomcat, JBoss, Jeus 등이 있다.

<br>

### Web Server와 WAS를 같이 사용하는 이유

1. **웹 애플리케이션 서버의 부담을 줄인다.**
   - WAS는 비즈니스 로직을 처리하는데 집중하고, 정적 컨텐츠는 Web Server에서 빠르게 클라이언트에게 응답한다.
   - 정적 컨텐츠까지 WAS에서 처리하면 부하가 커지고 그에 따라 처리가 지연되어 수행 시간이 늘어난다.

2. **SSL/TLS 보안 계층에 대해 Web Server에서 처리를 하여 WAS에 대한 보안을 강화한다.**
   - 이 역시 WAS에서 하면 되는거 아니냐고 생각할 수 있으나, Web Server에서 하면 WAS까지 가지 않고도 앞단에서 빠르게 처리가 가능하다.
3. **여러 대의 WAS 연결이 가능하다.**
   - 여러 대의 WAS를 띄우고 이에 대해 로드 밸런싱을 하기 위해 Web Server를 앞단에 둔다.
   - 특히나 대규모 트래픽 서비스의 경우 Web Server와 WAS를 분리하여 무중단 운영을 위한 장애 극복에 쉽게 대응할 수 있다.

<br>

---



## 3.2. Apache VS Nginx

### Apache HTTP Server

> Apache는 프로세스 / 스레드 기반 구조로 동작하는 웹 서버이다.

![](https://velog.velcdn.com/images/kylekim2123/post/d3b4d76b-798b-470f-80ea-49daa050c349/image.png)


**클라이언트로부터 요청이 들어오면 새 커넥션을 생성하기 위해 새로운 프로세스(혹은 스레드)를 할당**한다.

아래와 같은 두 가지 방식으로 프로세스(혹은 스레드)를 분배한다.

1. **Prefork 방식**
    - 하나의 클라이언트 요청에 하나의 자식 프로세스를 할당한다.
    - 따라서 요청이 늘어나면 프로세스도 함께 늘어난다.
    - 하나의 자식 프로세스 당 하나의 스레드를 가지며, 총 1024개의 자식 프로세스를 할당 가능하다.
2. **Worker 방식**
    - 하나의 요청에 하나의 프로세스를 할당하지 않고, 하나의 요청마다 자식 프로세스에 하나의 스레드를 새로 할당한다.
    - 하나의 자식 프로세스 당 여러 개의 스레드를 가진다.
    - 따라서 Prefork에 비해 비교적 메모리를 적게 사용한다.

<br>

Apache가 등장하고 시간이 지나면서 PC의 보급률이 높아져 점점 트래픽이 많아졌다.

클라이언트 요청 하나당 하나의 프로세스(혹은 스레드)를 생성하는 구조는 `C10K`의 문제에 직면하게 된다.

> `C10K` 문제란, `Connection 10,000개 문제`라는 의미로, 요청에 의해 생겨나는 커넥션들이 10,000개를 넘어가지 못하는 문제를 말한다. 
> 당시 CPU는 충분히 요청을 처리할 수 있었으나 Apache 구조 상, 한 개 커넥션 당 하나의 프로세스(스레드) 할당으로 인해 메모리가 버티지 못했다.

이러한 구조적인 문제를 해결하고자 2004년에 Nginx가 등장한다.

<br>

### Nginx

>  Nginx는 Event-Driven 기반 구조로 작동하는 웹 서버이다.

![](https://velog.velcdn.com/images/kylekim2123/post/0a9683a9-e1e7-4b62-a170-70a81dd3541c/image.png)


Apache와 달리 **하나의 고정된 프로세스만 생성하고, 새로 들어오는 클라이언트의 요청은 이벤트 핸들러를 이용해 비동기 방식으로 처리**한다.

요청이 늘어날 때 추가적으로 프로세스(혹은 스레드)를 할당하지 않기 때문에 메모리를 적게 사용한다.


**Master-Worker 방식**

- Nginx는 설정 파일을 읽고 Worker 프로세스를 생성하는 `Master 프로세스`와, 실제로 클라이언트의 요청을 처리하는 `Worker 프로세스`로 이루어진다.
- Nginx를 구동하면, Master 프로세스는 정해진 수만큼의 Worker 프로세스를 생성한다.
(**보통 CPU 코어 개수만큼 Worker 프로세스를 생성하여 각 코어의 컨텍스트 스위치 비용을 줄인다.**)
- Worker 프로세스는 Working Queue에 담긴 이벤트(ex. 커넥션 연결, 요청 처리, 커넥션 종료 등)들을 순차적으로 처리한다. 새로운 프로세스(혹은 스레드)를 생성하지 않기 때문에 이 Worker 프로세스는 끊임없이 작업을 수행한다.
- 따라서 커넥션마다 프로세스(혹은 스레드)를 할당하던 Apache와 달리 메모리를 적게 점유하면서 훨씬 효율적으로 요청을 처리한다.

<br>

하지만 Working Queue에 시간이 오래 걸리는 작업이 들어오면(ex. Disk I/O), Worker 프로세스가 해당 이벤트를 처리하는 동안 Queue에 쌓여있는 다른 이벤트들은 계속 대기하게 된다. 이는 전체적인 시간 지연을 가져올 수 있다.

따라서 Nginx에 `스레드 풀(Thread Pool)` 개념을 도입하여, 오래 걸리는 작업(ex. Disk I/O, File Read)를 처리하는 스레드 묶음을 따로 만들어 이를 해결한다. (1.7.11 버전 부터 도입)

![](https://velog.velcdn.com/images/kylekim2123/post/7aff8748-5833-4c8a-9405-38b90c09256d/image.png)


1. 오래 걸리는 작업은 Worker 프로세스에서 Task Queue로 이관된다.
2. Task Queue는 스레드 풀에 있는 스레드들 중 하나에 작업을 할당한다.
3. 작업이 완료되면 다시 Worker 프로세스에게 결과를 전달한다.

<br>

이런 개선의 과정을 통해 Apache에 비해 처리 가능한 동시 커넥션 양이 `100~1000배` 정도 증가하게 되었다.
또한 동일한 커넥션 일 때 Apache에 비해 약 `2배` 정도 속도가 향상되었다.

<br>

---



## 3.3. SSL/TLS offloading

- 요청과 응답의 보안성을 향상 시키기 위해 SSL/TLS 인증서를 이용한 HTTPS를 많이 사용하는데, 이 과정에서 SSL handshaking이 발생한다.
- SSL handshaking 과정은 생각보다 많은 리소스를 사용하게 된다. 따라서 이를 API 서버가 아닌 별도의 프록시 서버에서 담당하도록 위임한다.

![image](https://github.com/backend-team-study/cs-study/assets/49775540/7647a00f-fdca-4248-ad0b-900ff3b102bc)

- 프록시 서버는 일반 API 서버보다 SSL과 관련된 암호화와 복호화에 성능이 좋은 장치를 보통 사용한다.
- 프록시 서버를 거쳐서 복호화된 데이터는 HTTP 통신을 이용해 API 서버와 통신하므로, 기존에 HTTPS 만을 이용한 통신보다 속도가 빠르다.
- 이를 `SSL/TLS offloading` 전략이라고 부른다.

<br>

---



## 3.4. Reverse Proxy

- `프록시(Proxy)`란, 클라이언트와 서버 사이에 위치한 중계 서버로써 대리자 역할을 하는 것을 말한다.

- 클라이언트와 서버 사이에 프록시 서버를 두면, 보안이 강화되고 통신 성능이 높아진다는 장점이 있다.

- 크게 `포워드 프록시(Forward Proxy)`와 `리버스 프록시(Reverse Proxy)`로 나눌 수 있다.

<br>

### 포워드 프록시(Forward Proxy)

![](https://velog.velcdn.com/images/kylekim2123/post/f91f2700-50bd-4c3a-bec2-055e7036aca1/image.png)

**정의**

- `포워드 프록시`는 **클라이언트와 인터넷 사이**에 존재하는 프록시 서버를 의미한다.
- 클라이언트가 서버에 직접 요청하지 않고, 포워드 프록시 서버가 해당 요청을 받아서 서버에 전달한 후 그 응답 값을 클라이언트에게 전달한다.

**역할**

- 주로 `캐시(Cache)` 용도로 많이 사용한다.
- 기업에서는 내부망에 포워드 프록시를 두어 정해진 사이트만 연결하는 등의 `보안` 용도로도 사용한다.

<br>

### 리버스 프록시(Reverse Proxy)

![](https://velog.velcdn.com/images/kylekim2123/post/ac7978d2-89a3-4884-8105-7b5641708622/image.png)

**정의**

- `리버스 프록시`는 **인터넷과 서버 사이**에 존재하는 프록시 서버를 의미한다.
- 클라이언트의 요청을 서버가 직접 받지 않고, 리버스 프록시 서버가 해당 요청을 받아서 서버에 전달한 후 그 응답 값을 인터넷으로 전달한다. 클라이언트는 프록시 뒤의 서버의 존재를 모르게 된다.

**역할**

- `로드 밸런서`의 역할로 사용하여, 집중적으로 발생하는 부하를 여러 서버로 나눠 보낼 수 있다.
- 클라이언트의 요청을 나눠서 보낼 수 있으므로, `무중단 배포` 시 배포 중인 서버에 요청을 보내지 않도록 할 수 있다.
- 서버는 내부망에 넣고, 리버스 프록시 서버는 외부에 두어 `보안`을 강화시킬 수 있다.

<br>



---



## 3.5. Load Balancing

- 로드 밸런싱이란, 둘 혹은 셋 이상의 처리 장치와 저장 장치에 작업을 나누는 것을 의미한다. 가용성과 응답 시간의 최적화를 위해 사용한다.

- 로드 밸런서란, 다수의 클라이언트 요청을 다수의 웹 서버에 분산시키는 중간 장치를 의미하고 L2, L3, L4, L7 로드밸런서가 존재한다.

<br>

### L2, L3 로드밸런서

- L2, L3 스위치는 OSI 7계층 중, 2, 3계층에 해당하는 데이터 연결 계층과 네트워크 계층 단위에서의 로드밸런서를 의미한다.
- MAC 주소 혹은 IP 주소를 기준으로 부하를 분산한다.

<br>

### L4 로드밸런서

- L4 스위치는 OSI 7계층 중, 4계층에 해당하는 전송 계층 단위에서의 로드밸런서를 의미한다.
- IP와 Port를 기준으로 부하를 분산한다.
- L7에 비해 상대적으로 빠르나, 기능과 유연성의 면에서는 상대적으로 제한적이다.
- 온라인 게임, 스트리밍 서비스 등 실시간 트래픽 처리가 중요한 서비스에 적합하다.

<br>

### L7 로드밸런서

- L7 스위치는 OSI 7계층 중, 7계층에 해당하는 애플리케이션 계층 단위에서의 로드밸런서를 의미한다.
- IP, Port 이외에도 URI, Payload, Header, Cookie 등을 기준으로 부하를 분산한다.
- L4에 비해 상대적으로 느리나, 기능과 유연성의 면에서는 상대적으로 다양하다.
- 웹 서비스, API 게이트웨이, CDN 등의 애플리케이션 로드 밸런싱이 필요한 서비스에 적합하다.

<br>

L4는 단지 부하를 분산하는 용도라면, L7은 요청의 세부적인 사항을 두고 결제 서버, 회원가입 서버 등으로 분리하여 

가볍고 작은 단위로 여러 개의 서비스를 운영하고 요청을 각각의 서버에 분산하는 용도이다.

<br>

### 로드 밸런싱 알고리즘

1. **라운드 로빈 (Round Robin, RR)**
   - 로드밸런서가 다수의 서버에게 순서대로 요청을 할당하는 방법이다.
   - 가장 단순한 방법으로 서버군에 차례로 요청을 할당하여 분산한다. 다른 알고리즘에 비해 가장 빠르다.
   - 연결된 세션이 비교적 오래 사용되지 않는 경우에 채택하는 것이 좋다.
2. **최소 연결 (Least Connection)**
   - 로드밸런서가 서버에게 요청을 전달한 뒤, 클라이언트와 서버가 연결을 맺게되면 서로 커넥션을 생성한다.
   - 로드밸런서 또한 중간자로서 커넥션 정보를 갖는데, 이 커넥션 수를 기반으로 가장 커넥션이 적은 서버에게 요청을 전달하여 분산한다.
3. **가중치 (Ratio)**
   - 각 서버의 처리 능력을 고려하여, 할당할 수 있는 커넥션 비율을 미리 계산한다.
   - 로드밸런서가 각 서버의 커넥션 비율에 따라 요청을 분산한다.
4. **최소 응답 시간 (Fastest Response Time)**
   - 응답 속도가 가장 빠른 서버에게 우선적으로 할당하는 방법이다.
   - 특정 서버에 할당된 커넥션이 5개인데, 서버의 응답이 5개라면 -> 갖고 있는 커넥션에 대해 모두 응답하므로 성능이 충분하다고 판단한다.
   - 특정 서버에 할당된 커넥션이 10개인데, 서버의 응답이 5개라면 -> 현재 성능이 충분치 않아 제대로 응답하지 못하는 것으로 판단한다.
   - 따라서 제대로 응답하지 못한다고 판단하면 추가적인 요청에 대해 해당 서버로 보내지 않는다.

5. **해시 (Hash)**
   - 특정 기준을 잡아 특정 서버에 매핑하여 고정적으로 트래픽을 분산하는 방법이다.
   - 일반적으로 사용되는 기준은 클라이언트의 IP 주소가 된다.

<br>

---



## 3.6. Web Cache

- 캐시가 없다면 원하는 이미지 등의 파일을 서버에 요청할 때, 해당 파일의 헤더와 본문 부분을 요청할 때마다 새로 받아야한다.
- 그래서 데이터가 변경되지 않아도 계속 네트워크를 통해서 데이터를 새로 다운로드 받아야 한다.
- 따라서 로딩도 느리고, 돈도 많이 나가고, 사용자 경험도 나빠지는 문제가 발생한다.

<br>

캐시를 적용하면, 최초 요청한 파일을 캐시에 저장하고 이후 일정 시간 동안의 요청에 대해서는 캐시에서 응답을 할 수 있다.

이 경우, **데이터에 대한 로딩도 빨라지고 네트워크 사용량을 줄일 수 있어서, 최종적으로 사용자 경험이 개선**된다.

만약 일정 시간이 지나 캐시가 사제되면, 원래 서버를 통해 다시 데이터를 조회하여 캐시를 갱신하면 된다.

<br>

### 검증 헤더와 조건부 요청 1

캐시의 유효 시간이 초과하여 서버에 다시 요청하면 나타나는 두 가지 상황은 다음과 같다.

1. **서버에서 기존 데이터를 변경하였는가?**
   - 이 경우, 변경된 새로운 데이터를 다시 캐시로 받아오면 된다.
2. **서버에서 기존 데이터를 변경하지 않았는가?**
   - 이 경우, 캐시에 저장했던 데이터를 재사용하면 된다.
   - 다만, 캐시에 저장된 데이터와 서버에 저장된 데이터가 같은 데이터인지 어떻게 검증할까? 이를 아는 방법이 필요하다.
   - 이를 알기 위해 검증 헤더와 조건부 요청이라는 방법을 사용한다.

<br>

1. 서버는 `Last-Modified: 2023년 11월 10일 10:00:00` (검증헤더) 을 담아서 응답을 보내고 클라이언트는 그 정보를 캐시에 넣는다.

2. 로컬의 캐시에는 해당 데이터의 최종 수정일이 들어있게 된다. 그리고 60초가 지나서 캐시가 삭제된다. (cache-control 헤더의 max-age가 60)
3. 클라이언트가 다시 요청을 보낼 때, 캐시에 들어있던 Last-Modified 정보를 넣어서 요청을 보낸다.
   - `If-Modified-Since: 2023년 11월 10일 10:00:00` (조건부 요청)

4. 서버는 자신의 데이터 최종 수정일과, 클라이언트가 조건부 요청으로 보낸 데이터 최종 수정일을 비교한다.
   - 날짜와 시간이 같으면 서버에서 데이터를 변경하지 않았다는 뜻이므로, 클라이언트는 기존에 캐시에 있던 데이터를 재사용하면 된다.
5. 날짜와 시간이 같아서 변경되지 않았다는 점을 클라이언트에게 알리기 위해, 서버는 304 Not Modified를 응답한다. (HTTP 본문은 없음)
6. 클라이언트는 304 응답을 받으면 캐시 데이터를 사용하라는 뜻으로 인식하여, 캐시에 있는 데이터를 삭제하지 않고 재사용한다. (60초 재시작)

<br>

### 검증 헤더와 조건부 요청 2

- `검증 헤더` : 캐시 데이터와 서버 데이터가 같은지 검증하는 데이터
  - `Last-Modified`
  - `ETag`
- `조건부 요청` : 검증 헤더를 통한 조건에 따른 분기
  - `If-Modified-Since` : Last-Modified를 사용하여 조건을 분기한다.
  - `If-None-Match` : ETag를 사용하여 조건을 분기한다.
  - 조건을 만족하면 `200 OK`를 이용해 새로운 데이터를 전송한다. (캐시 갱신)
  - 조건을 만족하지 못하면 `304 Not Modified`를 이용해 캐시를 재사용하라고 알린다. (HTTP 본문 없이 헤더만 빠르게 전송)

<br>

**Last-Modified와 If-Modified-Since는 아래와 같은 단점이 존재한다.**

- 1초 미만의 단위로는 캐시 갱신 조정이 불가능하다.
- 날짜 기반의 로직을 사용하므로, 데이터를 수정해서 날짜는 다르나 데이터 수정 전후의 차이가 없는 경우에도 캐시를 갱신한다. (내용은 같아도 수정 일자가 다르므로 갱신했다고 판단)
- 이러한 단점들을 보완하기 위해 ETag를 사용한다.

<br>

### ETag와 If-None-Match

> 캐시 데이터에 임의의 고유한 버전 이름을 달아두는 방식

1. 서버가 요청에 대한 데이터를 응답할 때, `ETag: "v1.0"` 와 같은 형식으로 태그 이름을 지정하여 응답한다.

2. 클라이언트는 해당 데이터를 받아 캐시에 저장한다. 60초 후 캐시는 삭제된다.
3. 클라이언트는 데이터를 서버에 재요청한다. 이때 `If-None-Match: "v1.0"` 을 요청 헤더에 넣어 보낸다.
4. 서버는 자신의 데이터의 태그와, 클라이언트의 조건부 요청의 태그를 비교하여 다르면 데이터를 보낸다. 같으면 304를 보낸다.

<br>

**ETag 방식은 데이터 자체에 대한 해시를 통해 태그를 관리하기 때문에, 파일 자체가 같으면 언제 변경이 되었더라도 항상 해시값이 동일하다.**

따라서 Last-Modified 방식과 달리, 같은 내용을 유지한다면 언제나 같은 데이터 취급을 받는다.

결국은 캐시 제어 로직을 서버에서 완전히 관리하는 방식이므로, 클라이언트는 캐시 매커니즘을 모르게된다.

<br>

### 캐시 제어 헤더

> 서버에서 클라이언트에게 보내는 응답 코드에 담기는 캐시 관련 헤더

1. **Cache-Control**
   - `max-age` : 캐시 유효 시간을 지정한다. (초 단위)
   - `no-cache` : 데이터를 캐시해도 되나, 항상 원래 서버에 검증하고 사용한다. (조건부 요청을 통해 304를 받고 캐시를 사용해야한다.)
   - `no-store` : 민감성 데이터이므로 캐시에 저장하면 안된다. 메모리에서만 사용하고 빠르게 삭제해야한다.
   - `must-revalidate` : 캐시 만료 후 최초 조회 시, 원래 서버에 검증하고 사용한다. (원래 서버에 접근 실패 시 반드시 504 Gateway Timeout 오류가 발생해야한다.)
   - `public` : 응답을 public 캐시에 저장할 수 있다. (클라이언트와 중간 프록시들이 캐시 저장 가능)
   - `private` : 응답을 private 캐시에 저장할 수 있다. (클라이언트만 캐시 저장 가능)
   - `s-maxage` : 프록시 캐시에 적용되는 max-age를 의미한다. (프록시 캐시에서 얼마나 살아있을 것인가에 대한 것)
2. **Pragma**
   - `no-cache` : HTTP 1.0에 대한 하위호환이므로 거의 사용하지 않는다.
3. **Expires**
   - 캐시 만료일을 정확한 날짜로 지정한다.
   - HTTP 1.0에 대한 하위호환이므로 Cache-Control의 max-age가 더 권장된다. (함께 사용하면 Expires가 무시된다.)

<br>

### 캐시 무효화

> 웹 브라우저가 캐시를 자동으로 저장하는 것을 무효화하는 방식

- 캐시를 만들지 않더라도 브라우저가 자체적으로 캐시를 만드는 경우가 많다.

- 캐시 제어 헤더를 이용해서 캐시 무효화 전략을 사용할 수 있다.

  ```
  Cache-Control: no-cache, no-store, must-revalidate
  Pragma: no-cache
  ```

- no-cache가 있으므로 항상 원래 서버에 조건부 요청을 보낼 텐데, 왜 must-revalidate도 같이 넣는 것일까?
  - 만약 no-cache만 존재한다면, 원래 서버와 단절되어 접근할 수 없는 경우 프록시 캐시 서버에서 캐시 데이터를 200 상태 코드와 함께 클라이언트에게 반환하게 된다.
  - 하지만 캐시 데이터가 아닌, 반드시 최신화 된 데이터를 보여줘야 하는 경우(계좌 관련 등)는, 원래 서버와 통신을 반드시 해야 하므로 must-revalidate를 통해서 원래 서버와 통신이 단절되면 504 상태 코드를 통해 오류를 발생시켜야 하는 것이다.

<br>

---



## 3.7. URI

> Uniform Resource Identifier (통합 자원 식별자)

![image](https://github.com/backend-team-study/cs-study/assets/49775540/e99585e7-f43f-4693-8ba3-625c02f8cac6)

### URL

> Uniform Resource Locator

1. `scheme`
- 주로 프로토콜을 적는다. (http, https, ftp 등)
2. `userinfo`
   - URL에 사용자 정보를 포함해서 인증한다.
   - 거의 사용하지 않는다.
3. `host`
- 호스트명, 도메인명 또는 IP 주소를 직접 입력 가능하다.
4. `port`
- 포트번호이며, 일반적으로 생략한다.
5. `path`
   - 리소스의 경로이며, 계층적 구조를 띈다.
   - 예) /home/file1.jpg ⇒ home에 있는 file1.jpg 파일을 의미한다.
6. `query`
   - key = value의 형태이며, ?로 시작하고 &로 추가 가능하다.
   - query parameter, query string 등으로 불리며 웹 서버에 제공하는 파라미터이다.
7. `fragment`
   - html 내부 북마크 등에 사용한다.
   - 서버에 전송하는 정보가 아니다.

<br>

### URN

> Uniform Resource Name

- URL과 다른 점은, URN은 자원을 영구적이고 유일하게 식별할 수 있는 주소라는 점이다.
- URN에는 자원에 대한 접근 방법이나 웹 상의 위치가 표시되지 않는다. 단순히 자원 자체에 부여된 유일한 이름을 나타낸다.

![image](https://github.com/backend-team-study/cs-study/assets/49775540/3d4178c2-3288-4fc6-b723-4b211d5bba77)

<br>

---



## 3.8. Rest API

> REpresentational State Transfer API

URI를 통해 자원을 명시하고, HTTP Method를 통해 해당 자원에 대한 생성, 조회, 수정, 삭제 연산을 적용하는 것을 말한다.

- `URI` : 자원
- `HTTP Method` : 자원에 대한 행위
- `Payload` : 자원에 대한 행위의 내용

<br>

**장점**

- HTTP의 인프라를 그대로 사용하므로 이를 활용하여 기존 장점 + 추가적인 장점을 함께 가져갈 수 있다.
- HTTP를 따르는 모든 플랫폼에서 사용 가능하다.
- URI와 Method를 통해 의도하는 바를 명확하게 나타내므로 의미가 명시적이다.
- 클라이언트와 서버의 역할을 명확히 구분한다.

<br>

**단점**

- 표준이 존재하지 않아 별도의 표준화가 필요하다.
- HTTP Method의 형태가 제한적이라 모든 경우의 수에 대해 대비할 수 없다.
- 구형 브라우저에서 호환이 되지 않는 경우가 있다.

<br>

아래와 같은 제약 조건을 지켜야 한다.

- Client-Server
- Stateless
- Cache
- Uniform Interface
- Layered System
- Code-on-Demand (선택)

<br>

### Uniform Interface

가장 지켜지지 않는 부분이 Uniform Interface이다. 여기에는 세분화 된 4가지의 조건들이 있다.

- **Identification of resources**
  - URI를 통해 자원이 식별되어야 한다는 조건 (잘 지켜짐)
- **Manipulation of resources through representations**
  - 자원을 생성, 수정, 삭제할 때 HTTP 메시지에 표현을 전송해야 한다는 조건 (잘 지켜짐)
- **Self-descriptive messages**
  - 메시지는 스스로를 설명해야 한다는 조건 (잘 안지켜짐)
  - 보통 Json 데이터를 Rest API를 통해 주고 받는데, 이 때 Json 데이터에 대한 내용은 프론트엔드와 백엔드 담당자끼리 합의하여 함축된 의미를 가지는 단어들이 대부분이다. 즉 Json 데이터 자체가 해당 데이터의 의미를 명확하게 설명하지 못한다. 이는 Self-descriptive 하지 못한다.
  - 따라서 HTTP 메시지는 그 자체만으로도 모든 것을 완벽하게 설명할 수 있도록 명시적으로 작성해야 한다. 물론 현실적으로 어렵다.
- **Hypermedia as the engine of application state(HATEOAS)**
  - 애플리케이션의 상태는 하이퍼링크를 통해 전이되어야 한다는 조건 (잘 안지켜짐)
  - HTML의 경우는 HATEOAS를 만족한다. 왜냐면 anchor 태그를 통해 자원에 대해 하이퍼링크가 모두 명시되어있기 때문이다.
  - 하지만 Json의 경우는 자원들에 대해 해당 자원을 조회할 수 있는 하이퍼링크가 대부분 명시되어 있지 않다. 따라서 HATEOAS를 만족하지 못한다.
  - 따라서 조회하는 자원에 대해 하이퍼링크도 같이 담아서 응답해야 한다. 물론 현실적으로 어렵다.

<br>

로이 필딩이 Rest를 처음 만들게 된 이유는, 웹을 최대한 망가뜨리지 않고 어떻게 하면 독립적인 규약을 만들어낼 수 있을까에 대한 고민이었다.

따라서 로이 필딩의 의도에 따르면 Rest API는 최대한 독립적인 형태를 띄어야한다. 하지만 대부분 현재의 Rest라고 하는 API들은 Self-descriptive와 HATEOAS를 만족하지 못한다. 이는 메시지 자체, 데이터 자체만으로 스스로를 설명할 수 없고 스스로의 위치를 나타낼 수 없으므로 로이 필딩의 초기 의도와는 엄연히 다르게 되었다.

<br>

---



## 3.9. 질문 모음

1. Web Server와 WAS의 차이점에 대해 설명해주세요.
2. WAS만 사용해도 정적, 동적 컨텐츠에 대한 처리를 모두 할 수 있을텐데, Web Server를 추가적으로 사용하는 이유는 어떤게 있을까요?
3. Web Server에서 Apache와 Nginx의 차이점에 대해 설명해주세요.
4. SSL Offloading이란 무엇인가요? 적용 했을 때 어떤 장점이 있나요?
5. 프록시란 무엇인가요? 포워드 프록시와 리버스 프록시는 어떤 차이가 있나요? 각각 어떤 역할로 많이 사용하나요?
6. 로드밸런싱이란 무엇인가요? L4 로드밸런서와 L7 로드밸런서의 차이는 무엇인가요?
7. 로드밸런싱 알고리즘 중 라운드 로빈 방식과 최소 연결 방식에 대해 설명해주세요.
8. 웹 캐시를 사용한다면, 사용하지 않을 때에 비해 어떤 장점들이 있을까요?
9. 웹 캐시를 사용하기 위한 검증 헤더에서 Last-Modified와 ETag의 차이점에 대해 설명해주세요.
10. 웹 캐시 제어 헤더에서 `Cache-Control: no-cache`와 `Cache-Control: must-revalidate`는 어떤 차이가 있나요?
11. 웹 캐시를 사용하지 않으려 해도, 브라우저가 자체적으로 캐시를 저장하는 경우가 있는데, 이럴 때 캐시 무효화를 위해 어떤 헤더를 사용해볼 수 있을까요?
12. URL과 URN의 차이점에 대해 간단히 설명해주세요.
13. Rest API란 무엇인가요?
14. Rest API와 관련된 개념 중에 HATEOAS에 대해 아신다면 설명해주세요.
